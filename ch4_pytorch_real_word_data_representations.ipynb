{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch4_pytorch_real_word_data_representations.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wz1AGgqGavUm",
        "QFxDsbERa0CP"
      ],
      "authorship_tag": "ABX9TyOxGa+aPq4WDCbEfyIEuyvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/pytorch-book/blob/master/ch4_pytorch_real_word_data_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TJCw-wh6mg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import imageio\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNoZ2DH311KJ",
        "colab_type": "text"
      },
      "source": [
        "## Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPT-oubM7WET",
        "colab_type": "text"
      },
      "source": [
        "### Single image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VLJZ1zO2z71",
        "colab_type": "text"
      },
      "source": [
        "We need to be able to lead an image from common image formats and then transform the data into a tensor representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-ay1q1315Li",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2acdf91-3e2f-4048-d839-27a492ffa603"
      },
      "source": [
        "# Let's read a PNG image\n",
        "\n",
        "img_arr = imageio.imread('https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-dog/bobby.jpg')\n",
        "img_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28zKqYfJ5h7v",
        "colab_type": "text"
      },
      "source": [
        "What are these? The first number is the height, while the second is the width. The last one is the number of channels, Red, Green and Blue.\n",
        "\n",
        "So, for example, indexing `img_arr[0,0]` will return the RGB values for the top-leftmost pixel.\n",
        "\n",
        "We could convert this to a PyTorch tensor, but PyTorch requires this to be in the layout Channels x Height X Width, so we need to change the layout, using the `permute` method. In this case we need to put the RGB values first, while the other two dimensions can stay the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhxNZtAE4IV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = torch.from_numpy(img_arr)\n",
        "out = img.permute(2,0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyP-dUCA4sc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21c1549c-9d67-4e14-a55f-710ea1c50036"
      },
      "source": [
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 720, 1280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv957bMN7qig",
        "colab_type": "text"
      },
      "source": [
        "### Multiple images in a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj_TYQG57u3h",
        "colab_type": "text"
      },
      "source": [
        "When we have multiple images, we store them in a batch, a tensor with four dimensions: N x C x H x W (Number of images, channels, height and width).\n",
        "\n",
        "For a batch of 3 images, in RGB colour, each of 256 x 256 pixels, it can be created as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q__8jFYG8_o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 3\n",
        "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVgaiVMk9KBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-cats/'\n",
        "filenames = ['cat1.png', 'cat2.png', 'cat3.png']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MPd4kCN8zLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5c0209ff-ad9b-4854-f562-8be5c7f66183"
      },
      "source": [
        "# Loop through file names, load a NumPy array, convert to tensor with the right\n",
        "# permutation and add to the tensor\n",
        "for i, filename in enumerate(filenames):\n",
        "  filepath = data_dir + filename\n",
        "  print(filepath)\n",
        "  img_arr = imageio.imread(filepath)\n",
        "  img_torch = torch.from_numpy(img_arr)\n",
        "  out = img_torch.permute(2,0,1)\n",
        "  out = out[:3] # just keeping the R,G,B channels and ignore alpha\n",
        "  batch[i] = out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-cats/cat1.png\n",
            "https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-cats/cat2.png\n",
            "https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-cats/cat3.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVH_z3dv-UtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc386c4f-92d9-4012-f5ff-ad914d212e5a"
      },
      "source": [
        "# Now the batch has the required shapes\n",
        "batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 256, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knw6dkxg_lXu",
        "colab_type": "text"
      },
      "source": [
        "### Image data normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNP8B9WRAoSV",
        "colab_type": "text"
      },
      "source": [
        "Best training performance can be achieved when input data ranges from 0 to 1, or from -1 and 1. So we want to cast vectors to floating point and then normalise.\n",
        "\n",
        "There are 2 ways to do this:\n",
        "1. just divide the values of the pixels by 256\n",
        "2. calculate mean and standard deviation of channel data, and then scale it so that the output has zero mean and sd=1 across each channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AluHe90f_Ip4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First method\n",
        "batch_std1 = batch.float()\n",
        "batch_std1 /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqKuFFPBVzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d627cf12-4bb5-45ab-f36a-0ea68d7ac175"
      },
      "source": [
        "batch_std1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 256, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9R8cNJcBXZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cd4bd6c-774c-4393-a4d6-6233ca371078"
      },
      "source": [
        "# Check RBG values for the top-left pixel in first image of the batch\n",
        "batch_std1[0,:,0,0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6118, 0.5451, 0.5059])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fin2dEm1Ba4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second method\n",
        "batch_std2 = batch.float()\n",
        "number_of_channels = batch_std2.shape[1]\n",
        "for i in range(number_of_channels):\n",
        "  mean = torch.mean(batch_std2[:,i])\n",
        "  std = torch.std(batch_std2[:,i])\n",
        "  batch_std2[:,i] = (batch_std2[:,i] - mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ger5vHqQJEov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c924ac6-31e6-460a-ff59-c4e56b08a02d"
      },
      "source": [
        "# And again we can check the RGB values for the top-left pixel in first image of the batch\n",
        "batch_std2[0,:,0,0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1439, 0.4632, 0.7792])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSANcKnzOz05",
        "colab_type": "text"
      },
      "source": [
        "### 3D images (volumentric data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt5CjJ1RPJ72",
        "colab_type": "text"
      },
      "source": [
        "No real difference, we just have a 5th dimension, which we we can call `depth`. So the 5D tensor will have this dimension: N x C x D x H x W (Number of images, channels, depth, height and width)\n",
        "\n",
        "We can show how it works by loading an image in a specialised format\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4I0GDCtUMZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8j-dMEJP9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_path = '/content/drive/My Drive/ML/dicom'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tWNZt0DR1fU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cf4b951c-77b3-422d-df28-fc1e5e910988"
      },
      "source": [
        "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
        "vol_arr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading DICOM (examining files): 1/99 files (1.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/99 files (21.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/99 files (22.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/99 files (23.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/99 files (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/99 files (25.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/99 files (26.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/99 files (27.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/99 files (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/99 files (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/99 files (30.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/99 files (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/99 files (32.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33/99 files (33.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/99 files (34.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/99 files (35.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99 files (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/99 files (37.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/99 files (38.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/99 files (39.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/99 files (40.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/99 files (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/99 files (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/99 files (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/99 files (44.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/99 files (45.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/99 files (46.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/99 files (47.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b48/99 files (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99 files (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/99 files (50.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/99 files (51.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/99 files (52.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b53/99 files (53.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/99 files (54.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/99 files (55.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/99 files (56.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/99 files (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/99 files (58.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/99 files (59.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/99 files (60.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/99 files (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/99 files (62.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/99 files (63.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b64/99 files (64.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b65/99 files (65.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b66/99 files (66.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b67/99 files (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b68/99 files (68.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b69/99 files (69.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b70/99 files (70.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b71/99 files (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b72/99 files (72.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b73/99 files (73.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b74/99 files (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b75/99 files (75.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b76/99 files (76.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b77/99 files (77.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b78/99 files (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b79/99 files (79.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b80/99 files (80.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99 files (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b82/99 files (82.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b83/99 files (83.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b84/99 files (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b85/99 files (85.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b86/99 files (86.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b87/99 files (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b88/99 files (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b89/99 files (89.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90/99 files (90.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b91/99 files (91.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b92/99 files (92.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b93/99 files (93.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b94/99 files (94.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b95/99 files (96.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b96/99 files (97.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b97/99 files (98.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b98/99 files (99.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\n",
            "  Found 1 correct series.\n",
            "Reading DICOM (loading data): 10/99  (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99  (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/99  (63.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b90/99  (90.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTUNZYX7Wa9Q",
        "colab_type": "text"
      },
      "source": [
        "Here we have an array where the first dimension is the depth, the second is the height and the third is the width. Note that this is just one 3D image (provided with 99 separate files).\n",
        "\n",
        "We can convert this into a tensor, but we note that there is one dimension missing, i.e. the channels. In this case, we just have one channel (greyscale). We add it with the `unsqueeze` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhTsKEIZR6gy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f80a2dd-928e-47d0-b662-c8813a35ad5c"
      },
      "source": [
        "vol = torch.from_numpy(vol_arr).float()\n",
        "vol = torch.unsqueeze(vol, 0)\n",
        "vol.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 99, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irxnCwPGXBhf",
        "colab_type": "text"
      },
      "source": [
        "Now we could repeat the process for each image in the dataset, grouping them in a batch, i.e. a tensor with 5 dimension, of which the first one would be the 3D image number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz1AGgqGavUm",
        "colab_type": "text"
      },
      "source": [
        "## Tabular data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhIgf6k6bZyt",
        "colab_type": "text"
      },
      "source": [
        "Most common type of data (e.g. `csv`). Challenge is that this is often heterogenous data (integers, floats, text, etc.) which need to be converted into numeric tensors.\n",
        "\n",
        "Let's load a `csv` file and convert it into a tensor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4UeJk6icEEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "cd3788c0-225f-46cc-af8d-cf7e4d88f468"
      },
      "source": [
        "path = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/tabular-wine/winequality-white.csv'\n",
        "wineq_numpy = np.loadtxt(path, dtype=np.float32, delimiter=';', skiprows=1)\n",
        "wineq_numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
              "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
              "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
              "       ...,\n",
              "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
              "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
              "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hac_Z9_lclYY",
        "colab_type": "text"
      },
      "source": [
        "We can check that the file has been loaded correctly by loading the data separately as pandas, getting the column and seeing whether they match the dimensions of the array above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Mr8zcfckDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = pd.read_csv(path, sep=';').columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qEUGwqEczB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03c6cb22-96ce-45cb-fe8b-854858a4f904"
      },
      "source": [
        "wineq_numpy.shape, len(columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4898, 12), 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkKjGac2dTgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0f55ec0f-f87b-46d5-8fb4-507c8641e20a"
      },
      "source": [
        "columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
              "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
              "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv5egmiyed0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e209e38d-8e31-4e20-bffb-0ce78c91a019"
      },
      "source": [
        "# We can then convert the array into a tensor\n",
        "wineq = torch.from_numpy(wineq_numpy)\n",
        "wineq.shape, wineq.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 12]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL-j66qce9tm",
        "colab_type": "text"
      },
      "source": [
        "This tensor contains a column with the scores, though in general we'd want that to be in a separate tensor as it would be the set of labels used in training (aka the `target`). We do this below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnkGiTBCfc5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = wineq[:,:-1]\n",
        "target = wineq[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42TsfG69fc7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bbeb026-4df8-4517-b273-ecd53af320c4"
      },
      "source": [
        "data.shape, target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 11]), torch.Size([4898]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJWZPOMUf1EL",
        "colab_type": "text"
      },
      "source": [
        "Now we need to choose what we want to do to the labels. We could treat them as **categorical data** - in which case we would convert them to integers. The other option is to apply **one-hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvoniuQSfc9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9954f56a-9629-4da6-eec0-cbbd39d38494"
      },
      "source": [
        "# Convert the target into categorical data\n",
        "target = target.long()\n",
        "target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6, 6,  ..., 6, 7, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvjwKGe_fdAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da599eff-89d0-45a9-c9a5-3fc34aa737b2"
      },
      "source": [
        "# Apply one-hot encoding - this is done with the `scatter_` method.\n",
        "# Note: there are 4898 rows, and 10 labels. So, the resulting tensor needs to be\n",
        "# a 2D tensor (4898, 10). \n",
        "\n",
        "# We're starting from a 1D tensor and we want to go to a 2D tensor without changing\n",
        "# the content of the tensor\n",
        "\n",
        "target_unsqueezed = target.unsqueeze(1)\n",
        "target.shape, target_unsqueezed.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898]), torch.Size([4898, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_pzrGqafdCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "976f49c4-676e-4c21-e9db-6d60791d1056"
      },
      "source": [
        "# Then we can use the `scatter_` method. The first argument is the dimension we\n",
        "# apply this to, the second is a column tensor indicating the indices to scatter,\n",
        "# the third is the scalar to scatter (1 in this case).\n",
        "\n",
        "# First we define the empty tensor, with the same number of rows as the target, \n",
        "# but 10 columns, given that there are 10 different scores\n",
        "\n",
        "target_onehot = torch.zeros(target.shape[0], 10)\n",
        "\n",
        "# And then we populate it\n",
        "target_onehot.scatter_(1, target_unsqueezed, 1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUWwe4KrfdFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "339068bf-f875-4677-8d03-80d3c1db40be"
      },
      "source": [
        "target_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4898, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mks-g_gLkh_O",
        "colab_type": "text"
      },
      "source": [
        "We can use a boolean tensor to subset another one. In this case, we can set a threhold on the target variable, obtain a bool tensor and then use that to subset the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH6WMkDTpO0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad705cbb-16ba-4d50-b453-071fd068df84"
      },
      "source": [
        "subsetter = target <= 3\n",
        "subsetter.shape, subsetter.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898]), torch.bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5ru1D3pVRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c953cb3c-22e3-46eb-d9f1-15114f8a0108"
      },
      "source": [
        "bad_wine = data[subsetter]\n",
        "data.shape, bad_wine.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 11]), torch.Size([20, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxDsbERa0CP",
        "colab_type": "text"
      },
      "source": [
        "## Time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vC19DyIp5rf",
        "colab_type": "text"
      },
      "source": [
        "The objective in this case is to take the time fields in tabular data and use it to add a time dimension, i.e. going from a 2D tensor to a 3D one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOBfB9kmp5Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a dataset of bike rentals over two years\n",
        "path = 'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/bike-sharing-dataset/hour-fixed.csv'\n",
        "bikes_numpy = np.loadtxt(\n",
        "    path,\n",
        "    dtype=np.float32,\n",
        "    delimiter=\",\",\n",
        "    skiprows=1,\n",
        "    converters={1: lambda x: float(x[8:10])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRr_dG7Nt_Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3c66c6bb-18e5-4667-f17d-a078b9608177"
      },
      "source": [
        "columns = pd.read_csv(path, sep=',').columns\n",
        "columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
              "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
              "       'casual', 'registered', 'cnt'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRG6Y4lVt_Jm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fb092a5-07ec-44b2-a3d5-41fe8689e280"
      },
      "source": [
        "bikes_numpy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17520, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wpzAMgTuPw9",
        "colab_type": "text"
      },
      "source": [
        "There are 17,520 entries, with data across 17 dimensions. There is one entry per hour over a period of 2 years.\n",
        "\n",
        "Suppose now we want to reshape the data to have one collection per day (24 hours). In this case we want a tensor with these dimensions:\n",
        "- Number of collections: 730 (number of days in 2 years)\n",
        "- Number of data point per collection: 24 (number of hours in a day)\n",
        "- Number of dimensions, or channels per data point: 17\n",
        "\n",
        "We can do this using `view`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cjvS5AMuIva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bikes = torch.from_numpy(bikes_numpy)\n",
        "daily_bikes = bikes.view(-1, 24, bikes.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54AAWr-Ct_MG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45d696cb-ed54-49fc-eb29-c7b5b99a328b"
      },
      "source": [
        "daily_bikes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 24, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rfJmzEfxWBc",
        "colab_type": "text"
      },
      "source": [
        "The only issue is that now we have N (sequences) x L (hours) x C (channels), but the desidered ordering is N x C x L. So we need to transpose the last two dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUi31pN8xqJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6da26e68-a0cc-4dbb-8f51-b62e8ff147ef"
      },
      "source": [
        "daily_bikes = daily_bikes.transpose(2,1)\n",
        "daily_bikes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 17, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_h7CBesx2t9",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's apply one-hot encoding to one of the categorical values, i.e. `wheathersit` which is in position 9. This can take four values, from 0 to 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuagBpGnx1zX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88de847e-71f6-44f5-df4e-7e710b451f80"
      },
      "source": [
        "# First let's extract this and check its dimensions\n",
        "weather = daily_bikes[:,9,:]\n",
        "weather.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rItilraKySbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e69763a-f9a1-4c30-8399-c636c04c26f0"
      },
      "source": [
        "# We actually want it to have shape 730 x 4 (categories) x 24, so we need to add a dimension\n",
        "# but without adding any data. Also, we reduce the values by 1, as the weather rating go from 1 to 4\n",
        "# and we need it to be 0-based\n",
        "weather_unsqueezed = weather.long().unsqueeze(1) - 1\n",
        "weather_unsqueezed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 1, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hVOzLm6yu6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49214ca9-8f9b-4270-e216-eb5b4c9a5c7e"
      },
      "source": [
        "# Now we can apply the one-hot encoding. First define the target tensor\n",
        "weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\n",
        "weather_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 4, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfeMv8eBy-2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c850d197-546a-4e5a-d1c3-8b402e855acc"
      },
      "source": [
        "# And finally we can use scatter\n",
        "weather_onehot.scatter_(1, weather_unsqueezed, 1.0)\n",
        "weather_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 4, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lc_lRP9zJq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49c5c8a0-009c-463c-9a73-e8b1159b5a9c"
      },
      "source": [
        "# The last step is to concatenate this with the orginal dataset\n",
        "daily_bikes_onehot = torch.cat((daily_bikes, weather_onehot), dim=1)\n",
        "daily_bikes_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([730, 21, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XR5_YWoa3Xu",
        "colab_type": "text"
      },
      "source": [
        "## Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILwt8Ot1p0Ph",
        "colab_type": "text"
      },
      "source": [
        "Goal is turning text into a tensor of numbers, in line with the other cases. \n",
        "\n",
        "Essentially, whether we operate at character level or word level, the technique is the same: we use one-hot encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30feiYmvtHjv",
        "colab_type": "text"
      },
      "source": [
        "### Character level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5NfeUb3w4Ts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d44b3a05-7067-463a-e42e-79aeb838d81c"
      },
      "source": [
        "# Let's read some text\n",
        "!curl  https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/jane-austen/1342-0.txt >> /content/sample_data/jane.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  694k  100  694k    0     0  1286k      0 --:--:-- --:--:-- --:--:-- 1283k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWv3llDJrtps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "410102f8-8e82-431f-f291-edfa02325f3e"
      },
      "source": [
        "with open('/content/sample_data/jane.txt', 'r', encoding='utf8') as f:\n",
        "  text = f.read() \n",
        "\n",
        "len(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1408380"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV5N_NjCxwOc",
        "colab_type": "text"
      },
      "source": [
        "We now need to parse through all the characters in the text and provide a one-hot encoding for each of them. Each character will be represented by a vector of length equal to all the different characters in the encoding. \n",
        "\n",
        "To start, we just focus on one line of text, splitting on `n`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMpeAvWJt3CX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92e5db94-9509-4487-8f06-78cb8e9ccde3"
      },
      "source": [
        "lines = text.split('\\n')\n",
        "line = lines[200]\n",
        "line"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnzajzJN1F6L",
        "colab_type": "text"
      },
      "source": [
        "Now, we first create a tensors of zeros which would hold the one-hot encoded vectors (based on characters). The dimensions are:\n",
        "- length of line: we need a vector for each character in the line\n",
        "- number of possible characters: using ASCII, this would be 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvoiFvcX1XkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f41f9df9-7e36-491c-d5cc-26cb937936ae"
      },
      "source": [
        "ASCII_size = 128\n",
        "letter_t = torch.zeros(len(line), ASCII_size)\n",
        "letter_t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([70, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CtpWgvW20bA",
        "colab_type": "text"
      },
      "source": [
        "To do the one-hot encoding by character, we enumerate through the (lower-cased) string. For each character, we get its corresponding ASCII value using the `ord` function and then populate the corresponding position in the tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaTm8Bub2kwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, letter in enumerate(line.lower().strip()):\n",
        "  letter_index = ord(letter) if ord(letter) < 128 else 0 # to only encode \"normal characters\"\n",
        "  letter_t[i][letter_index] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83gMDW__8L7V",
        "colab_type": "text"
      },
      "source": [
        "### Word level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAcu9TAn-j2r",
        "colab_type": "text"
      },
      "source": [
        "In this case, we want to apply one-hot encoding to individual words. We proceed as follows:\n",
        "\n",
        "- We first define a function to split strings into lists of words (removing punctuation)\n",
        "- We use the function on the entire corpus to build a dictionary to provide the positioning index for each word\n",
        "- We iterate through the words in the chosen sentence (also split into individual words) and populate the tensor of one-hot encoded vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7Lbjzs8Tut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(string):\n",
        "  punctuation = \".,;:!?“\\'_-\"\n",
        "  words = string.lower().replace('\\n', ' ').split()\n",
        "  clean_words = [word.strip(punctuation) for word in words]\n",
        "  return clean_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_sgemMHuLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2fae23c8-7e72-4eb9-c4c9-f32da8752278"
      },
      "source": [
        "# Clean and split our chosen sentence\n",
        "words_to_encode = clean_text(line)\n",
        "len(words_to_encode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPK5qla7Hwfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17b85462-f685-4311-9f72-e0ed4ee91357"
      },
      "source": [
        "# Now we apply the same function to the corpus, define the set of unique words,\n",
        "# and build a lookup dictionary which we can use in the one-hot encoding\n",
        "unique_words = sorted(set(clean_text(text)))\n",
        "len(unique_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icuzBQZ7UDln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22646a87-3126-42d4-c532-94e79398287a"
      },
      "source": [
        "dictionary = {word: i for i, word in enumerate(unique_words)}\n",
        "len(dictionary), dictionary['impossible']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8222, 3820)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ1sN3P6UGSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We have a dictionary with 8,222 unique words. Now we can define a tensor for\n",
        "# the one-hot encoding of our sentence. Accordingly the dimensions will be:\n",
        "# - 11: the length of our sentence\n",
        "# - 8222: the length of the dictionary  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Ko1KUlUfz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9edace20-8ecf-4a97-c376-104d2a34b8d3"
      },
      "source": [
        "word_t = torch.zeros(len(words_to_encode), len(dictionary))\n",
        "word_t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 8222])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM8HNhHVUg7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "554d5261-f84a-4075-e25e-8292bd1a8d0e"
      },
      "source": [
        "# And finally we can do the one-hot encoding\n",
        "for i, word in enumerate(words_to_encode):\n",
        "  word_index = dictionary[word]\n",
        "  word_t[i][word_index] = 1\n",
        "  print(f'#{i} {word_index}: {word}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0 3820: impossible\n",
            "#1 4893: mr\n",
            "#2 879: bennet\n",
            "#3 3820: impossible\n",
            "#4 8007: when\n",
            "#5 3731: i\n",
            "#6 428: am\n",
            "#7 5045: not\n",
            "#8 228: acquainted\n",
            "#9 8082: with\n",
            "#10 3609: him\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI4NvaiqWOsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}