{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch8_using_convolutions_to_generalise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/rBlHdUrzKYvW9YKwBezo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29528de6374942028483881c1c9a3ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd083954d3fa446ea9456cd1fb49046d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4000a00dc8be4199a17d06c0c4564c98",
              "IPY_MODEL_07957e234ac64995804fbf3bda0b32d0"
            ]
          }
        },
        "cd083954d3fa446ea9456cd1fb49046d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4000a00dc8be4199a17d06c0c4564c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cfe5165751ca442a84767133775cff67",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8da6000242064b44bb5ed97806b5bf71"
          }
        },
        "07957e234ac64995804fbf3bda0b32d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5379e50559df48d49de1f9e0c14933eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 8945210.02it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13745fa3ef9548d690fa9b7044675089"
          }
        },
        "cfe5165751ca442a84767133775cff67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8da6000242064b44bb5ed97806b5bf71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5379e50559df48d49de1f9e0c14933eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13745fa3ef9548d690fa9b7044675089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/pytorch-book/blob/master/ch8_using_convolutions_to_generalise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXO7gTlUpPAN",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FibRln8RpYKt",
        "colab_type": "text"
      },
      "source": [
        "We re-create the same datasets which we used in the previous chapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GSXP7CNpWuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Qd-7PopdG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "29528de6374942028483881c1c9a3ee3",
            "cd083954d3fa446ea9456cd1fb49046d",
            "4000a00dc8be4199a17d06c0c4564c98",
            "07957e234ac64995804fbf3bda0b32d0",
            "cfe5165751ca442a84767133775cff67",
            "8da6000242064b44bb5ed97806b5bf71",
            "5379e50559df48d49de1f9e0c14933eb",
            "13745fa3ef9548d690fa9b7044675089"
          ]
        },
        "outputId": "c46e4f18-a1c4-436a-ab30-405576cb0620"
      },
      "source": [
        "## Download files locally\n",
        "!mkdir /content/cifar10/\n",
        "from torchvision import datasets\n",
        "data_path_for_saving = '/content/cifar10/'\n",
        "cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29528de6374942028483881c1c9a3ee3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/cifar10/cifar-10-python.tar.gz to /content/cifar10/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRkfSrmI4rS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "tensor_cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=False,\n",
        "                                      transform=transforms.ToTensor())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIiKiXEpppMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53b23f71-4c4e-4ba8-8544-d4bdf0637683"
      },
      "source": [
        "imgs = torch.stack([item_t for item_t, _ in tensor_cifar10], dim = 3)\n",
        "reshaped = imgs.view(3,-1) # keep the first dimension, squash all others into a single dim\n",
        "print(reshaped.shape)\n",
        "\n",
        "means = reshaped.mean(dim=1) # mean of the first dimension (i.e. the three channels)\n",
        "stds = reshaped.std(dim=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 51200000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSFOF1ZgpfEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we have the information to do the normalisation, as follows.\n",
        "# Note that we `compose` transformations together\n",
        "transformed_cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=False,\n",
        "                                       transform = transforms.Compose([\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((means[0], means[1], means[2]),\n",
        "                                                              (stds[0], stds[1], stds[2]))                              \n",
        "                                       ]))\n",
        "\n",
        "transformed_cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=False,\n",
        "                                       transform = transforms.Compose([\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((means[0], means[1], means[2]),\n",
        "                                                              (stds[0], stds[1], stds[2]))                              \n",
        "                                       ]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIhrWZzpnuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We just want to keep class 0 and 2. Remember that the __getitem__ method returns\n",
        "# both the item and its label\n",
        "class_mapping = {0: 0, 2: 1}\n",
        "\n",
        "cifar2 = [(item, class_mapping[label])\n",
        "          for item, label in tensor_cifar10 if label in [0,2]]\n",
        "cifar2_val = [(item, class_mapping[label])\n",
        "          for item, label in tensor_cifar10_val if label in [0,2]] "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A_1DEwy5KMO",
        "colab_type": "text"
      },
      "source": [
        "## Intro to convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egCbMsrN5Yvc",
        "colab_type": "text"
      },
      "source": [
        "_**Note:** We now have the same datasets as before (`cifar2` and `cifar2_val`) at our disposal._\n",
        "\n",
        "_**Note 2:** my focus for this chapter is on how to subclass the `nn` module, hence will cover convolutions only at a high level._\n",
        "\n",
        "Convolutions are needed to deliver locality and translation invariance.\n",
        "\n",
        "Convolutions for a 2D image are the scalar (dot) product of a weight matrix (the _kernel_) with every neighbourhood in the input.\n",
        "\n",
        "So, if we have kernel as follows:\n",
        "\n",
        "```\n",
        "weight = torch.tensor([[w00, w01, w02],\n",
        "                       [w10, w11, w12],\n",
        "                       [w20, w21, w22]])\n",
        "```\n",
        "and  a 1-channel MxN image:\n",
        "```\n",
        "image = torch.tensor([[i00, i01, i02, i03, ..., i0N],\n",
        "                      [i10, i11, i12, i13, ..., i1N],\n",
        "                      [i20, i21, i22, i23, ..., i2N],\n",
        "                      [i30, i31, i32, i33, ..., i3N],\n",
        "                      ...\n",
        "                      [iM0, iM1m iM2, iM3, ..., iMN]])\n",
        "```\n",
        "The the output at, say, `1,1` can be calculated as follows (without bias):\n",
        "```\n",
        "o11 = i11 * w00 + i12 * w01 + i13 * w02 +\n",
        "      i21 * w10 + i22 * w11 + i23 * w12 +\n",
        "      i31 * w20 + i32 * w21 + i33 * w22\n",
        "```\n",
        "By repeating this calculation for each input of the image (i.e. by translating the kernel on all input locations) we obtain an output image where each location will be expressed as a weighted sum of its immediate vicinity (depending on the size of the kernel).\n",
        "\n",
        "Note that we do not know the weights in advance: these are randomly initialised at the beginning and then learned through model training. Note also that the same weights are using across the whole image, so they have a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\n",
        "\n",
        "This also mean that we have many fewer parameters compared with a fully connected model which accepts 1D inputs. The number of parameters will no longer depend on the number of pixels in the image, but rather on:\n",
        "- the size of the convolution kernel\n",
        "- how many convolution filters (or output channels) there are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_heeutxB76U",
        "colab_type": "text"
      },
      "source": [
        "## Using convolutions\n",
        "\n",
        "The `torch.nn` module provides convolutions for 1, 2 and 3 dimensions: `nn.Conv1D` for time series (and text), `nn.Conv2D` for images and `nn.Conv3D` for volumes or video. \n",
        "\n",
        "To `nn.Conv2D` we need to provide the following arguments:\n",
        "- the number of input features (or channels, as there will be more than 1 value per pixel)\n",
        "- the number of output features\n",
        "- the size of the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9SOs29A5Xr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC21BH_OQE1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0040fd48-0226-444c-81d6-daec59bfdf2d"
      },
      "source": [
        "# In our case we have 3 channels, so the input features will be 3. We choose an\n",
        "# arbitrary output of 16 features and a 3x3 kernel\n",
        "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
        "conv"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5lo3qUNQXe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c76abf4-b298-4d6c-fbdf-81f908aeaeae"
      },
      "source": [
        "# And these are the dimensions of weight and bias\n",
        "conv.weight.shape, conv.bias.shape\n",
        "# 16 output features, 3 input features (channel) and a 3x3 kernel. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmRHFsXgTKUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a15251fc-03d0-4c1f-df7d-a39c17e6a123"
      },
      "source": [
        "# Let's apply the convolution to one of the images in the dataset and compare it\n",
        "# with the original one. Let's get the image first:\n",
        "img, _ = cifar2[0]\n",
        "\n",
        "# Need to add the zero-th batch dimension\n",
        "output = conv(img.unsqueeze(0))\n",
        "\n",
        "# Let's check the dimensions - it looks like we've lost 2 pixels from each dimension...\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1pxfRBUk1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "8c3e9446-1fd8-4197-da1a-072ee5476889"
      },
      "source": [
        "# Displaying the two images reveals what has happened:\n",
        "from matplotlib import pyplot as plt \n",
        "f, axarr = plt.subplots(2, sharex=True, sharey=True)\n",
        "axarr[0].imshow(output[0,0].detach(), cmap='gray')\n",
        "axarr[1].imshow(img[0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3L5XgLpYEfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fdff5a63-c28d-4299-da59-ea4af895bc1d"
      },
      "source": [
        "# We can prevent this from happening by having some padding, which uses `ghost` pixels\n",
        "# on the sides of the image.\n",
        "conv_padded = nn.Conv2d(3,16, kernel_size=3, padding=1)\n",
        "output_padded = conv_padded(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output_padded.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcuLjtH7aghZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9c317384-d13d-4937-ef7a-8d5f8fe5e2f7"
      },
      "source": [
        "f, axarr = plt.subplots(1,2, sharex=True, sharey=True)\n",
        "axarr[0].imshow(output_padded[0,0].detach(), cmap='gray')\n",
        "axarr[1].imshow(img[0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLOiwxHbeaSe",
        "colab_type": "text"
      },
      "source": [
        "A way to make sure the convolutions can really pick up the salient features of an image, is to put these through some downsampling, so that only the \"strongest\" features, i.e. the features that better characterise an image would survive.\n",
        "\n",
        "This is achieved by using a `max pooling` operation, where only the features with the maximum value in an area are kept. This reduces the size of the image, keeping only the most salient features. \n",
        "\n",
        "Let's apply this to our image and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDfk2WABXfjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43ab95ed-10a4-4938-8b91-e7f4201ff9c6"
      },
      "source": [
        "max_pool = nn.MaxPool2d(2) # using 2 halves the size of the image\n",
        "output_pool = max_pool(output_padded)\n",
        "output_padded.shape, output_pool.shape\n",
        "# We can see the size has been halve. And we can display all three images"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 16, 32, 32]), torch.Size([1, 16, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroggD7Fe_aK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "63b342be-a277-4006-c67c-85ef1b71c9a8"
      },
      "source": [
        "f, axarr = plt.subplots(1,3)\n",
        "axarr[0].imshow(img[0].detach(), cmap='gray')\n",
        "axarr[1].imshow(output_padded[0,0].detach(), cmap='gray')\n",
        "axarr[2].imshow(output_pool[0,0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Note that the last image is half the size of the other two (much more \"pixelated\"), \n",
        "# but also note that compared to the output of the convolution, it appears \n",
        "# to give more weight to certain aspects of the image."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKm_XgXZi5Tw",
        "colab_type": "text"
      },
      "source": [
        "## Putting everything together in a model\n",
        "\n",
        "We can now put the elements we have into a single model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrqHFgkilhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2), # reducing the image size by half\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2), # we now have a 8x8 image\n",
        "#   nn.Flatten(), # not used to motivate the subclassing of nn.Module - see below\n",
        "    nn.Linear(8 * 8 * 8, 32), # 8 channels from previous convolution, 8x8 image from MaxPool\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32,2)) # output of dimension 2, the class probabilities\n",
        "\n",
        "# Note that at some point, we need to go from a 2D tensor to a 1D one."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gtrD0LjqMvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "bd78c50f-10ef-46e5-ae17-9580cfbc572e"
      },
      "source": [
        "# What happens if we try to get an output from this?\n",
        "out = model(img.unsqueeze(0))\n",
        "out"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-570a209e3988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What happens if we try to get an output from this?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 8], m2: [512 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67s3S48zrMYK",
        "colab_type": "text"
      },
      "source": [
        "This is due to the fact that we're not reshaping from a 8-channel 8x8 image to a 512-element 1D vector. We could do it with `view`, but in this case within the `nn.Sequential` module, we don't see the output of the previous model, so we can't use `view`.\n",
        "\n",
        "Let's use this as an opportunity to show that we can write our own module, which will enable us to do this reshaping. We do this by subclassing `nn.Module`. \n",
        "\n",
        "_(**Note:** since version 1.3 pytorch has the module `nn.Flatten()` which would solve this problem, but we go ahead with the alternative appraoch anyway)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6UI1Z1vq9Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define all modules in the class, and then put them together in the `forward`\n",
        "# method. Note that there is no `backward` method, as pytorch uses autograd.\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.Tanh()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.Tanh()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.act3 = nn.Tanh()\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv1(out)))\n",
        "    out = out.view(-1, 8 * 8 * 8) # the reshaping we needed before\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return(out)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFJdSStXvXC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce5caa4d-c995-4fea-cd97-50ac40aa41f1"
      },
      "source": [
        "# We can also see how big (i.e. how many parameters this model has)\n",
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otXjkzX5v3xH",
        "colab_type": "text"
      },
      "source": [
        "Note that here we're using the model to define the entire network, but it could also be defined to a building block that could be used as a model within `nn.Sequential`\n",
        "\n",
        "Now, there is a way to write the above model in a more compact way, avoiding having to write out the module that have no parameters, like `nn.Tanh` and `nn.MaxPool2D`. For that we can use their functional form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fljIq6vrNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * 8) # the reshaping we needed before\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return(out)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDEL8Oxx8UX",
        "colab_type": "text"
      },
      "source": [
        "We can now train the model as usual. We will need to first write a training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMefg8Tgx1g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs) # calculate predictions\n",
        "      loss = loss_fn(outputs, labels) # calculate loss\n",
        "      optimizer.zero_grad() # get rid of the previous gradient\n",
        "      loss.backward() # backprop\n",
        "      optimizer.step() # update parameters\n",
        "      loss_train += loss.item()\n",
        "    \n",
        "    if epoch ==1 or epoch % 10 == 0:\n",
        "      print(f'{datetime.datetime.now()} - Epoch: {epoch} - Loss: {loss_train / len(train_loader)}') # average loss per batch"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLvPgDFuz3jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And finally we can train. Running on CPU this will take a while\n",
        "import torch.utils.data\n",
        "import torch.optim as optim"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV3QXXVk0Izw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the loader\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87e_0irV0RuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise the model, definte optimizer and the loss\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfN9UBwK0ZYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "ed9589b2-4692-4373-a786-303d72f89dc4"
      },
      "source": [
        "# Train (commented out as it takes a long time on CPU)\n",
        "# training_loop(\n",
        "#     n_epochs = 100,\n",
        "#     optimizer = optimizer,\n",
        "#     model = model,\n",
        "#     loss_fn = loss_fn,\n",
        "#     train_loader = train_loader\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-14 16:34:59.480955 - Epoch: 1 - Loss: 0.6714251307165546\n",
            "2020-08-14 16:35:39.461101 - Epoch: 10 - Loss: 0.4660154134984229\n",
            "2020-08-14 16:36:23.742914 - Epoch: 20 - Loss: 0.35437491279878436\n",
            "2020-08-14 16:37:07.903650 - Epoch: 30 - Loss: 0.33049961886588175\n",
            "2020-08-14 16:37:52.014473 - Epoch: 40 - Loss: 0.3133844162817973\n",
            "2020-08-14 16:38:36.654306 - Epoch: 50 - Loss: 0.30199066744108866\n",
            "2020-08-14 16:39:20.821056 - Epoch: 60 - Loss: 0.29209849295342805\n",
            "2020-08-14 16:40:05.448901 - Epoch: 70 - Loss: 0.27837342612302984\n",
            "2020-08-14 16:40:50.029361 - Epoch: 80 - Loss: 0.26692975772793887\n",
            "2020-08-14 16:41:34.388436 - Epoch: 90 - Loss: 0.2520826905491246\n",
            "2020-08-14 16:42:18.656572 - Epoch: 100 - Loss: 0.24048625170045598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLL9Acf-_ZeI",
        "colab_type": "text"
      },
      "source": [
        "Let's now check the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmItAiiE0qO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv86MHPp_i8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        outputs = model(imgs)\n",
        "        _ , predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f\"Accuracy: {name, correct / total}\")      "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6RVUiBvAEsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad1b88a6-7002-4dee-85f9-68188e060149"
      },
      "source": [
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: ('train', 0.5)\n",
            "Accuracy: ('val', 0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atA5S9J9A6xB",
        "colab_type": "text"
      },
      "source": [
        "Finally we can see how to save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9HGia_GAIg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model\n",
        "!mkdir \"/content/models\"\n",
        "path = '/content/models/'\n",
        "torch.save(model.state_dict(), path + 'birds_vs_airplanes.pt')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDS5QqZCBMaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64eece12-2cad-4388-f239-8069fddf83a6"
      },
      "source": [
        "# loading the model\n",
        "loaded_model = Net() # to load the model, you first need to initialise it with\n",
        "                     # exactly the same definition!\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(path + 'birds_vs_airplanes.pt'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNGmUbDMCewv",
        "colab_type": "text"
      },
      "source": [
        "## Training on the GPU\n",
        "\n",
        "We can now train the model on the GPU to make it faster. First let's check that the GPU is indeed available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gZWOMBeBij9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06659e94-5b96-4155-9b61-36c0fef1cdae"
      },
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f'Training on device {device}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SuUcIB5DZw3",
        "colab_type": "text"
      },
      "source": [
        "To train on the GPU we need to move our tensors there using the `Tensor.to` method. So our training loop changes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0IZ9BKvCsZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  print(device)\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs) # calculate predictions\n",
        "      loss = loss_fn(outputs, labels) # calculate loss\n",
        "      optimizer.zero_grad() # get rid of the previous gradient\n",
        "      loss.backward() # backprop\n",
        "      optimizer.step() # update parameters\n",
        "      loss_train += loss.item()\n",
        "    \n",
        "    if epoch ==1 or epoch % 10 == 0:\n",
        "      print(f'{datetime.datetime.now()} - Epoch: {epoch} - Loss: {loss_train / len(train_loader)}') # average loss per batch"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZb1dricDxwl",
        "colab_type": "text"
      },
      "source": [
        "Apart from that change, everything stays the same. So let's train again, and see how much faster it goes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym7iA4kWD5K6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5021a27e-559f-47da-bb27-12f1fde8ce35"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device) # Need to move the model to the GPU as well\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "2020-08-19 09:57:32.830245 - Epoch: 1 - Loss: 0.6834329556507669\n",
            "2020-08-19 09:57:39.451271 - Epoch: 10 - Loss: 0.4792278281822326\n",
            "2020-08-19 09:57:46.760501 - Epoch: 20 - Loss: 0.3627285306241102\n",
            "2020-08-19 09:57:54.091123 - Epoch: 30 - Loss: 0.33240907890781474\n",
            "2020-08-19 09:58:01.414226 - Epoch: 40 - Loss: 0.307209140413506\n",
            "2020-08-19 09:58:08.755706 - Epoch: 50 - Loss: 0.2859120364211927\n",
            "2020-08-19 09:58:16.060504 - Epoch: 60 - Loss: 0.270072672207644\n",
            "2020-08-19 09:58:23.393800 - Epoch: 70 - Loss: 0.25513639049545217\n",
            "2020-08-19 09:58:30.783896 - Epoch: 80 - Loss: 0.24408595880885034\n",
            "2020-08-19 09:58:38.176649 - Epoch: 90 - Loss: 0.23355080087663263\n",
            "2020-08-19 09:58:45.525967 - Epoch: 100 - Loss: 0.22380357681755808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIRqeMDE1rI",
        "colab_type": "text"
      },
      "source": [
        "And we will need to make the same changes to validate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iPJJC_EjE9nw",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEqLXD5IE-2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        imgs = imgs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        model = model.to(device=device)\n",
        "        outputs = model(imgs)\n",
        "        _ , predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f\"Accuracy: {name, correct / total}\")    "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLGsLbJzFLs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "379be845-facc-44c5-b333-46b2afd4cb24"
      },
      "source": [
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: ('train', 0.91)\n",
            "Accuracy: ('val', 0.8845)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnlEwC76FTGZ",
        "colab_type": "text"
      },
      "source": [
        "Finally, there is a **complication** when loading network weights: pytorch will attempt to load the weights to the save device it was saved from. We have 2 options:\n",
        "1. move the model to the CPU before saving\n",
        "2. tell pytorch to override the device info when loading weights, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ1Gu0NVFJBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4accb256-77fe-4c6f-a1a4-4745893a202d"
      },
      "source": [
        "loaded_model = Net().to(device=device)\n",
        "loaded_model.load_state_dict(torch.load(path + 'birds_vs_airplanes.pt', map_location=device))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl5VVykhGFff",
        "colab_type": "text"
      },
      "source": [
        "## Model design\n",
        "\n",
        "We can explore ways in which we can modify the architecture of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76iHZsHJQ3n8",
        "colab_type": "text"
      },
      "source": [
        "### Width\n",
        "\n",
        "This is the number of neurons per layer, or channels per convolution. This is done by specifying a larger number of output channels in the first convolution and increase the subsequent layears accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTYdQKWGDMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetWidth(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 16 * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return output_pool"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPrFvrhuSKHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Even better, we can specify the number of channels in the init, so we do not \n",
        "# need to update the class everywhere whenever we want to change the number of channels\n",
        "class NetWidth(nn.Module):\n",
        "  def __init__(self, n_channels=32):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_channels, n_channels // 2, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear((n_channels // 2) * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, self.n_channels * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d578R7mDS0Xa",
        "colab_type": "text"
      },
      "source": [
        "Increasing the number of channels increases the number of parameters, thus increasing the _capacity_ of the model. Note that this will make it easier for the model to train, but it will also increase the likelihood of overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAT5SW7USzSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = NetWidth()\n",
        "model2 = NetWidth(n_channels=64)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkDSaHs1UBYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "838afb3d-f41d-4a7f-e838-266c7d78da52"
      },
      "source": [
        "sum(p.numel() for p in model1.parameters())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzJYDGsUFBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a6965e2-9b2e-4a43-fdfc-3c0f1bc8cbf2"
      },
      "source": [
        "sum(p.numel() for p in model2.parameters())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85890"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXeMt3hMUP95",
        "colab_type": "text"
      },
      "source": [
        "### Regularisation\n",
        "\n",
        "Regularisation helps the model to converge and generalise, i.e. it reduces the risk of overfitting. There are three main ways in which we can achieve regularisation:\n",
        "- Weight penalties\n",
        "- Dropout\n",
        "- Batch normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1cURCUbU95v",
        "colab_type": "text"
      },
      "source": [
        "#### _Weight penalties_\n",
        "\n",
        "This is done by adding a regularisation term to the loss. The penalty is on larger weight values. This makes the loss have a smoother topography, and there's less to gain from fitting individual samples.\n",
        "\n",
        "In pytorch we can implement regularisation pretty easily by adding a term to the loss. This changes the training loop as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fMcVgZIUMY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  print(device)\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs) # calculate predictions\n",
        "      loss = loss_fn(outputs, labels) # calculate loss\n",
        "\n",
        "      # Adding l1 or l2 penalty to loss\n",
        "      l2_lambda = 0.001\n",
        "      l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
        "      # or\n",
        "      # l1_norm = sum(p.abs().sum for p in model.parameters())\n",
        "      loss = loss + l2_lambda + l2_norm\n",
        "\n",
        "      optimizer.zero_grad() # get rid of the previous gradient\n",
        "      loss.backward() # backprop\n",
        "      optimizer.step() # update parameters\n",
        "      loss_train += loss.item()\n",
        "    \n",
        "    if epoch ==1 or epoch % 10 == 0:\n",
        "      print(f'{datetime.datetime.now()} - Epoch: {epoch} - Loss: {loss_train / len(train_loader)}') # average loss per batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04lomRJ6W-gy",
        "colab_type": "text"
      },
      "source": [
        "Note however that the SGD optimizer already has a `weight_decay` parameters which corresponds to `2 * lambda`, and it directly perform weight decay during the update. It is fully equivalent to adding the L2 norm of weights to the loss, without the need for accumulating terms in the loss and involving autograd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xbhPwiQYICE",
        "colab_type": "text"
      },
      "source": [
        "#### _Dropout_\n",
        "\n",
        "This basically involves zeroing out a random fraction of outputs from neurons across the network, where the randomisation happens at each training iteration. Effectively, this procedure generates slighly different models with different neuron topologies at each iteration.\n",
        "\n",
        "It's easy to implement this in pytorch. In the case of convolutions we can use the `nn.Dropout2d` or `nn.Dropout3d` modules. We can change the model as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq4RnqyjYF9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetDropout(nn.Module):\n",
        "  def __init__(self, n_channels=32):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1)\n",
        "    self.conv1_dropout = nn.Dropout2d(p=0.4) # p is the fractions of neurons being swiched off\n",
        "    self.conv2 = nn.Conv2d(n_channels, n_channels // 2, kernel_size=3, padding=1)\n",
        "    self.conv2_dropout = nn.Dropout2d(p=0.4) # also here\n",
        "    self.fc1 = nn.Linear((n_channels // 2) * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = self.conv1_dropout(out) # and we add it here...\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = self.conv2_dropout(out) # ...as well as here\n",
        "    out = out.view(-1, self.n_channels // 2 * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsUxXIbEoCer",
        "colab_type": "text"
      },
      "source": [
        "Dropout is **normally active during traininig** but not during **evaluation, where it's bypassed**. This is controlled through the `train` property of the `Dropout` module. Pytorch lets us switch between the two modalities by calling\n",
        "- `model.train()`\n",
        "or\n",
        "- `model.eval()`\n",
        "on any `nn.Model` subclass. The call will be automatically replicated on the submodules so that if `Dropout` is among them, it will behave accordingly in subsequent forward and backward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHmK0v2Px6VS",
        "colab_type": "text"
      },
      "source": [
        "#### _Batch normalisation_\n",
        "\n",
        "The main idea is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution.\n",
        "Batch normalisation shifts and scales an intermediate input using the mean and standard deviation collected at the intermediate location over the samples of the minibatch.\n",
        "\n",
        "Batch normalisation should reduce the need for dropout. This is done in pytorch by using the modules `nn.BatchNorm1d`, `nn.BatchNorm2d` and `nn.BatchNorm3d`, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qESMbLbnpIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetBatch(nn.Module):\n",
        "  def __init__(self, n_channels=32):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1)\n",
        "    self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_channels) # add batch normalisation module here...\n",
        "    self.conv2 = nn.Conv2d(n_channels, n_channels // 2, kernel_size=3, padding=1)\n",
        "    self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_channels) # ...also here\n",
        "    self.fc1 = nn.Linear((n_channels // 2) * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = self.conv1_batchnorm(out) # and we add it here...\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = self.conv2_batchnorm(out) # ...as well as here\n",
        "    out = out.view(-1, self.n_channels // 2 * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUuu_kEkzks6",
        "colab_type": "text"
      },
      "source": [
        "`model.train()` and `model.eval()` are used in the same way as for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9bOmv_Bzy-8",
        "colab_type": "text"
      },
      "source": [
        "### Depth\n",
        "\n",
        "Depth allows a model to deal with hierarchical information when we need to understand the context in order to say something about some input.\n",
        "\n",
        "But depth comes with some challenges, which prevented deep learning models from reaching 20 or more layers until late 2015. Depth makes training harder to converge. The main problem was the _vanishing gradient_ due to the fact that many small numbers are being multiplied with each other.\n",
        "\n",
        "However, there are now techniques that can be used to go around this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FpApDMa3bQU",
        "colab_type": "text"
      },
      "source": [
        "#### _Skip connections_\n",
        "\n",
        "This is a way to deal with vanishing gradient problem. The idea is quite simple: skip connections. This is nothing but the addition of the input to the output of a block of layers. This architecture (known as _residual networks_ or ResNets) could be implemented like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBTA8dOkzj7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetDepth(nn.Module):\n",
        "  def __init__(self, n_channels=32):\n",
        "    super().__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_channels, n_channels // 2, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(n_channels // 2, n_channels // 4, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear((n_channels // 4) * 4 * 4, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "    out1 = out # saving the output to a separate variable...\n",
        "    out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) # ...and adding it here \n",
        "    out = out.view(-1, self.n_channels // 2 * 4 * 4)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOwK0nD82uM",
        "colab_type": "text"
      },
      "source": [
        "It has been observed that skip connections have a beneficial effect on convergence especially in the initial phases of training.\n",
        "\n",
        "It's also possible to create sub-modules and then combining them together into more complex networks (see page 227-228 for an example)\n",
        "\n",
        "Finally, a comment about __initialisation.__ This is one of the important tricks in training neural networks. We note that the default weight initialisation in pytorch is not ideal. [See issue](https://github.com/pytorch/pytorch/issues/18182) to see whether this feature has been fixed / updated."
      ]
    }
  ]
}