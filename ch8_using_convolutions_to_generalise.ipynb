{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch8_using_convolutions_to_generalise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XXO7gTlUpPAN"
      ],
      "authorship_tag": "ABX9TyOXgw59RBF9rQEe9XO7imbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c0d69be0aaf4f478eaa30d448cf2c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_133bc5fb908448198c28ce223f2d75f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13a86a1ed7c84516af50480675637157",
              "IPY_MODEL_d85084359c6e441b97304cd7730979b9"
            ]
          }
        },
        "133bc5fb908448198c28ce223f2d75f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13a86a1ed7c84516af50480675637157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f41c167a2026418a9d368afa61327b99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a512919e66d4f8b86c7b637bb81b865"
          }
        },
        "d85084359c6e441b97304cd7730979b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a1050f8e01646afb730917e0be200ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:08&lt;00:00, 19117883.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_572074a9fe0d4b20a51d1ba747593850"
          }
        },
        "f41c167a2026418a9d368afa61327b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a512919e66d4f8b86c7b637bb81b865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a1050f8e01646afb730917e0be200ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "572074a9fe0d4b20a51d1ba747593850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/pytorch-book/blob/master/ch8_using_convolutions_to_generalise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXO7gTlUpPAN",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FibRln8RpYKt",
        "colab_type": "text"
      },
      "source": [
        "We re-create the same datasets which we used in the previous chapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GSXP7CNpWuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Qd-7PopdG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "5c0d69be0aaf4f478eaa30d448cf2c3f",
            "133bc5fb908448198c28ce223f2d75f6",
            "13a86a1ed7c84516af50480675637157",
            "d85084359c6e441b97304cd7730979b9",
            "f41c167a2026418a9d368afa61327b99",
            "5a512919e66d4f8b86c7b637bb81b865",
            "6a1050f8e01646afb730917e0be200ca",
            "572074a9fe0d4b20a51d1ba747593850"
          ]
        },
        "outputId": "9018a5f9-be0f-4949-a5c6-6dc7073566af"
      },
      "source": [
        "## Download files locally\n",
        "!mkdir /content/cifar10/\n",
        "from torchvision import datasets\n",
        "data_path_for_saving = '/content/cifar10/'\n",
        "cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c0d69be0aaf4f478eaa30d448cf2c3f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/cifar10/cifar-10-python.tar.gz to /content/cifar10/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRkfSrmI4rS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor_cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "tensor_cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=False,\n",
        "                                      transform=transforms.ToTensor())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIiKiXEpppMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "699503ae-02c5-4882-b208-79d8da894ef9"
      },
      "source": [
        "imgs = torch.stack([item_t for item_t, _ in tensor_cifar10], dim = 3)\n",
        "reshaped = imgs.view(3,-1) # keep the first dimension, squash all others into a single dim\n",
        "print(reshaped.shape)\n",
        "\n",
        "means = reshaped.mean(dim=1) # mean of the first dimension (i.e. the three channels)\n",
        "stds = reshaped.std(dim=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "torch.Size([3, 51200000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSFOF1ZgpfEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we have the information to do the normalisation, as follows.\n",
        "# Note that we `compose` transformations together\n",
        "transformed_cifar10 = datasets.CIFAR10(data_path_for_saving, train=True, download=False,\n",
        "                                       transform = transforms.Compose([\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((means[0], means[1], means[2]),\n",
        "                                                              (stds[0], stds[1], stds[2]))                              \n",
        "                                       ]))\n",
        "\n",
        "transformed_cifar10_val = datasets.CIFAR10(data_path_for_saving, train=False, download=False,\n",
        "                                       transform = transforms.Compose([\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((means[0], means[1], means[2]),\n",
        "                                                              (stds[0], stds[1], stds[2]))                              \n",
        "                                       ]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIhrWZzpnuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We just want to keep class 0 and 2. Remember that the __getitem__ method returns\n",
        "# both the item and its label\n",
        "class_mapping = {0: 0, 2: 1}\n",
        "\n",
        "cifar2 = [(item, class_mapping[label])\n",
        "          for item, label in tensor_cifar10 if label in [0,2]]\n",
        "cifar2_val = [(item, class_mapping[label])\n",
        "          for item, label in tensor_cifar10_val if label in [0,2]] "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A_1DEwy5KMO",
        "colab_type": "text"
      },
      "source": [
        "## Intro to convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egCbMsrN5Yvc",
        "colab_type": "text"
      },
      "source": [
        "_**Note:** We now have the same datasets as before (`cifar2` and `cifar2_val`) at our disposal._\n",
        "\n",
        "_**Note 2:** my focus for this chapter is on how to subclass the `nn` module, hence will cover convolutions only at a high level._\n",
        "\n",
        "Convolutions are needed to deliver locality and translation invariance.\n",
        "\n",
        "Convolutions for a 2D image are the scalar (dot) product of a weight matrix (the _kernel_) with every neighbourhood in the input.\n",
        "\n",
        "So, if we have kernel as follows:\n",
        "\n",
        "```\n",
        "weight = torch.tensor([[w00, w01, w02],\n",
        "                       [w10, w11, w12],\n",
        "                       [w20, w21, w22]])\n",
        "```\n",
        "and  a 1-channel MxN image:\n",
        "```\n",
        "image = torch.tensor([[i00, i01, i02, i03, ..., i0N],\n",
        "                      [i10, i11, i12, i13, ..., i1N],\n",
        "                      [i20, i21, i22, i23, ..., i2N],\n",
        "                      [i30, i31, i32, i33, ..., i3N],\n",
        "                      ...\n",
        "                      [iM0, iM1m iM2, iM3, ..., iMN]])\n",
        "```\n",
        "The the output at, say, `1,1` can be calculated as follows (without bias):\n",
        "```\n",
        "o11 = i11 * w00 + i12 * w01 + i13 * w02 +\n",
        "      i21 * w10 + i22 * w11 + i23 * w12 +\n",
        "      i31 * w20 + i32 * w21 + i33 * w22\n",
        "```\n",
        "By repeating this calculation for each input of the image (i.e. by translating the kernel on all input locations) we obtain an output image where each location will be expressed as a weighted sum of its immediate vicinity (depending on the size of the kernel).\n",
        "\n",
        "Note that we do not know the weights in advance: these are randomly initialised at the beginning and then learned through model training. Note also that the same weights are using across the whole image, so they have a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\n",
        "\n",
        "This also mean that we have many fewer parameters compared with a fully connected model which accepts 1D inputs. The number of parameters will no longer depend on the number of pixels in the image, but rather on:\n",
        "- the size of the convolution kernel\n",
        "- how many convolution filters (or output channels) there are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_heeutxB76U",
        "colab_type": "text"
      },
      "source": [
        "## Using convolutions\n",
        "\n",
        "The `torch.nn` module provides convolutions for 1, 2 and 3 dimensions: `nn.Conv1D` for time series (and text), `nn.Conv2D` for images and `nn.Conv3D` for volumes or video. \n",
        "\n",
        "To `nn.Conv2D` we need to provide the following arguments:\n",
        "- the number of input features (or channels, as there will be more than 1 value per pixel)\n",
        "- the number of output features\n",
        "- the size of the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9SOs29A5Xr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC21BH_OQE1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e1851c0-9928-453a-d323-abdca1dbe94a"
      },
      "source": [
        "# In our case we have 3 channels, so the input features will be 3. We choose an\n",
        "# arbitrary output of 16 features and a 3x3 kernel\n",
        "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
        "conv"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5lo3qUNQXe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "681de9f8-cddb-4643-f9fb-fc64a94d3920"
      },
      "source": [
        "# And these are the dimensions of weight and bias\n",
        "conv.weight.shape, conv.bias.shape\n",
        "# 16 output features, 3 input features (channel) and a 3x3 kernel. "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmRHFsXgTKUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6c3a8e8-d2d0-4c83-b9b5-1ea639cba353"
      },
      "source": [
        "# Let's apply the convolution to one of the images in the dataset and compare it\n",
        "# with the original one. Let's get the image first:\n",
        "img, _ = cifar2[0]\n",
        "\n",
        "# Need to add the zero-th batch dimension\n",
        "output = conv(img.unsqueeze(0))\n",
        "\n",
        "# Let's check the dimensions - it looks like we've lost 2 pixels from each dimension...\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1pxfRBUk1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "62b7f31c-df38-4596-dd6d-a4f0827bbd3b"
      },
      "source": [
        "# Displaying the two images reveals what has happened:\n",
        "from matplotlib import pyplot as plt \n",
        "f, axarr = plt.subplots(2, sharex=True, sharey=True)\n",
        "axarr[0].imshow(output[0,0].detach(), cmap='gray')\n",
        "axarr[1].imshow(img[0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3L5XgLpYEfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "930ec143-93db-4b54-aa55-d60da17d750f"
      },
      "source": [
        "# We can prevent this from happening by having some padding, which uses `ghost` pixels\n",
        "# on the sides of the image.\n",
        "conv_padded = nn.Conv2d(3,16, kernel_size=3, padding=1)\n",
        "output_padded = conv_padded(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output_padded.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcuLjtH7aghZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9551ce38-f01e-4481-cfd3-cd59862af68f"
      },
      "source": [
        "f, axarr = plt.subplots(1,2, sharex=True, sharey=True)\n",
        "axarr[0].imshow(output_padded[0,0].detach(), cmap='gray')\n",
        "axarr[1].imshow(img[0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLOiwxHbeaSe",
        "colab_type": "text"
      },
      "source": [
        "A way to make sure the convolutions can really pick up the salient features of an image, is to put these through some downsampling, so that only the \"strongest\" features, i.e. the features that better characterise an image would survive.\n",
        "\n",
        "This is achieved by using a `max pooling` operation, where only the features with the maximum value in an area are kept. This reduces the size of the image, keeping only the most salient features. \n",
        "\n",
        "Let's apply this to our image and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDfk2WABXfjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79e73b86-7510-4a1d-d10f-fd14c3d4dd92"
      },
      "source": [
        "max_pool = nn.MaxPool2d(2) # using 2 halves the size of the image\n",
        "output_pool = max_pool(output_padded)\n",
        "output_padded.shape, output_pool.shape\n",
        "# We can see the size has been halve. And we can display all three images"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 16, 32, 32]), torch.Size([1, 16, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroggD7Fe_aK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "834f5f4b-326d-436c-f335-de0b31f0fbbe"
      },
      "source": [
        "f, axarr = plt.subplots(1,3)\n",
        "axarr[0].imshow(img[0].detach(), cmap='gray')\n",
        "axarr[1].imshow(output_padded[0,0].detach(), cmap='gray')\n",
        "axarr[2].imshow(output_pool[0,0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Note that the last image is half the size of the other two (much more \"pixelated\"), \n",
        "# but also note that compared to the output of the convolution, it appears \n",
        "# to give more weight to certain aspects of the image."
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKm_XgXZi5Tw",
        "colab_type": "text"
      },
      "source": [
        "## Putting everything together in a model\n",
        "\n",
        "We can now put the elements we have into a single model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrqHFgkilhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2), # reducing the image size by half\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2), # we now have a 8x8 image\n",
        "#   nn.Flatten(), # not used to motivate the subclassing of nn.Module - see below\n",
        "    nn.Linear(8 * 8 * 8, 32), # 8 channels from previous convolution, 8x8 image from MaxPool\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32,2)) # output of dimension 2, the class probabilities\n",
        "\n",
        "# Note that at some point, we need to go from a 2D tensor to a 1D one."
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gtrD0LjqMvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "a75d5681-08d7-4992-d448-4b6f0f432187"
      },
      "source": [
        "# What happens if we try to get an output from this?\n",
        "out = model(img.unsqueeze(0))\n",
        "out"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-570a209e3988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What happens if we try to get an output from this?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 8], m2: [512 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67s3S48zrMYK",
        "colab_type": "text"
      },
      "source": [
        "This is due to the fact that we're not reshaping from a 8-channel 8x8 image to a 512-element 1D vector. We could do it with `view`, but in this case within the `nn.Sequential` module, we don't see the output of the previous model, so we can't use `view`.\n",
        "\n",
        "Let's use this as an opportunity to show that we can write our own module, which will enable us to do this reshaping. We do this by subclassing `nn.Module`. \n",
        "\n",
        "_(**Note:** since version 1.3 pytorch has the module `nn.Flatten()` which would solve this problem, but we go ahead with the alternative appraoch anyway)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6UI1Z1vq9Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define all modules in the class, and then put them together in the `forward`\n",
        "# method. Note that there is no `backward` method, as pytorch uses autograd.\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.Tanh()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.Tanh()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.act3 = nn.Tanh()\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv1(out)))\n",
        "    out = out.view(-1, 8 * 8 * 8) # the reshaping we needed before\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return(out)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFJdSStXvXC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "213611ad-6845-49b6-d3c3-fdbd2c6f1394"
      },
      "source": [
        "# We can also see how big (i.e. how many parameters this model has)\n",
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otXjkzX5v3xH",
        "colab_type": "text"
      },
      "source": [
        "Note that here we're using the model to define the entire network, but it could also be defined to a building block that could be used as a model within `nn.Sequential`\n",
        "\n",
        "Now, there is a way to write the above model in a more compact way, avoiding having to write out the module that have no parameters, like `nn.Tanh` and `nn.MaxPool2D`. For that we can use their functional form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fljIq6vrNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * 8) # the reshaping we needed before\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return(out)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDEL8Oxx8UX",
        "colab_type": "text"
      },
      "source": [
        "We can now train the model as usual. We will need to first write a training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMefg8Tgx1g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs) # calculate predictions\n",
        "      loss = loss_fn(outputs, labels) # calculate loss\n",
        "      optimizer.zero_grad() # get rid of the previous gradient\n",
        "      loss.backward() # backprop\n",
        "      optimizer.step() # update parameters\n",
        "      loss_train += loss.item()\n",
        "    \n",
        "    if epoch ==1 or epoch % 10 == 0:\n",
        "      print(f'{datetime.datetime.now()} - Epoch: {epoch} - Loss: {loss_train / len(train_loader)}') # average loss per batch"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLvPgDFuz3jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And finally we can train. Running on CPU this will take a while\n",
        "import torch.utils.data\n",
        "import torch.optim as optim"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV3QXXVk0Izw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the loader\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87e_0irV0RuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise the model, definte optimizer and the loss\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfN9UBwK0ZYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "ed9589b2-4692-4373-a786-303d72f89dc4"
      },
      "source": [
        "# Train\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-14 16:34:59.480955 - Epoch: 1 - Loss: 0.6714251307165546\n",
            "2020-08-14 16:35:39.461101 - Epoch: 10 - Loss: 0.4660154134984229\n",
            "2020-08-14 16:36:23.742914 - Epoch: 20 - Loss: 0.35437491279878436\n",
            "2020-08-14 16:37:07.903650 - Epoch: 30 - Loss: 0.33049961886588175\n",
            "2020-08-14 16:37:52.014473 - Epoch: 40 - Loss: 0.3133844162817973\n",
            "2020-08-14 16:38:36.654306 - Epoch: 50 - Loss: 0.30199066744108866\n",
            "2020-08-14 16:39:20.821056 - Epoch: 60 - Loss: 0.29209849295342805\n",
            "2020-08-14 16:40:05.448901 - Epoch: 70 - Loss: 0.27837342612302984\n",
            "2020-08-14 16:40:50.029361 - Epoch: 80 - Loss: 0.26692975772793887\n",
            "2020-08-14 16:41:34.388436 - Epoch: 90 - Loss: 0.2520826905491246\n",
            "2020-08-14 16:42:18.656572 - Epoch: 100 - Loss: 0.24048625170045598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLL9Acf-_ZeI",
        "colab_type": "text"
      },
      "source": [
        "Let's now check the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmItAiiE0qO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv86MHPp_i8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        outputs = model(imgs)\n",
        "        _ , predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f\"Accuracy: {name, correct / total}\")      "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6RVUiBvAEsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5f6017d1-99b8-4921-ce9c-0a71aaea879c"
      },
      "source": [
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: ('train', 0.8992)\n",
            "Accuracy: ('val', 0.879)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atA5S9J9A6xB",
        "colab_type": "text"
      },
      "source": [
        "Finally we can see how to save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9HGia_GAIg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model\n",
        "!mkdir \"/content/models\"\n",
        "path = '/content/models/'\n",
        "torch.save(model.state_dict(), path + 'birds_vs_airplanes.pt')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDS5QqZCBMaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23a42ea9-5cd8-4c8a-d4f9-ded323374e02"
      },
      "source": [
        "# loading the model\n",
        "loaded_model = Net() # to load the model, you first need to initialise it with\n",
        "                     # exactly the same definition!\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(path + 'birds_vs_airplanes.pt'))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNGmUbDMCewv",
        "colab_type": "text"
      },
      "source": [
        "## Training on the GPU\n",
        "\n",
        "We can now train the model on the GPU to make it faster. First let's check that the GPU is indeed available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gZWOMBeBij9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b979823-ea0b-4e4a-c293-2a96e4aa3087"
      },
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f'Training on device {device}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SuUcIB5DZw3",
        "colab_type": "text"
      },
      "source": [
        "To train on the GPU we need to move our tensors there using the `Tensor.to` method. So our training loop changes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0IZ9BKvCsZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  print(device)\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs) # calculate predictions\n",
        "      loss = loss_fn(outputs, labels) # calculate loss\n",
        "      optimizer.zero_grad() # get rid of the previous gradient\n",
        "      loss.backward() # backprop\n",
        "      optimizer.step() # update parameters\n",
        "      loss_train += loss.item()\n",
        "    \n",
        "    if epoch ==1 or epoch % 10 == 0:\n",
        "      print(f'{datetime.datetime.now()} - Epoch: {epoch} - Loss: {loss_train / len(train_loader)}') # average loss per batch"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZb1dricDxwl",
        "colab_type": "text"
      },
      "source": [
        "Apart from that change, everything stays the same. So let's train again, and see how much faster it goes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym7iA4kWD5K6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "2c510d0c-070f-43a0-89fa-a1288f57ec8c"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device) # Need to move the model to the GPU as well\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "2020-08-14 17:43:43.428740 - Epoch: 1 - Loss: 0.6838089284623504\n",
            "2020-08-14 17:43:46.942467 - Epoch: 10 - Loss: 0.4714757557127886\n",
            "2020-08-14 17:43:50.869793 - Epoch: 20 - Loss: 0.36084253298249214\n",
            "2020-08-14 17:43:54.835164 - Epoch: 30 - Loss: 0.32321396299228544\n",
            "2020-08-14 17:43:58.794928 - Epoch: 40 - Loss: 0.30528587368643206\n",
            "2020-08-14 17:44:02.727775 - Epoch: 50 - Loss: 0.2913887944001301\n",
            "2020-08-14 17:44:06.689945 - Epoch: 60 - Loss: 0.2774290480431478\n",
            "2020-08-14 17:44:10.603068 - Epoch: 70 - Loss: 0.26580145784244413\n",
            "2020-08-14 17:44:14.528947 - Epoch: 80 - Loss: 0.2524760164273013\n",
            "2020-08-14 17:44:18.481933 - Epoch: 90 - Loss: 0.23991774914750627\n",
            "2020-08-14 17:44:22.447318 - Epoch: 100 - Loss: 0.23043173921715682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIRqeMDE1rI",
        "colab_type": "text"
      },
      "source": [
        "And we will need to make the same changes to validate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iPJJC_EjE9nw",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEqLXD5IE-2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        imgs = imgs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        model = model.to(device=device)\n",
        "        outputs = model(imgs)\n",
        "        _ , predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f\"Accuracy: {name, correct / total}\")    "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLGsLbJzFLs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "726bd47d-45af-4db8-9800-44d0d65a9650"
      },
      "source": [
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: ('train', 0.9089)\n",
            "Accuracy: ('val', 0.881)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnlEwC76FTGZ",
        "colab_type": "text"
      },
      "source": [
        "Finally, there is a **complication** when loading network weights: pytorch will attempt to load the weights to the save device it was saved from. We have 2 options:\n",
        "1. move the model to the CPU before saving\n",
        "2. tell pytorch to override the device info when loading weights, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ1Gu0NVFJBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09339f76-f58e-4c7d-a2ce-b9eab7e0aff2"
      },
      "source": [
        "loaded_model = Net().to(device=device)\n",
        "loaded_model.load_state_dict(torch.load(path + 'birds_vs_airplanes.pt', map_location=device))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl5VVykhGFff",
        "colab_type": "text"
      },
      "source": [
        "## Model design"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTYdQKWGDMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}